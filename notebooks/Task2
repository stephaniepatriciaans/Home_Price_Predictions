# ================================================
# Home Price Modeling (Clean Version with Comments)
# ================================================
# - Loads cleaned_enhanced.csv
# - Preprocesses numeric + categorical features
# - Tunes Ridge (with OHE) and RandomForest (with ordinal encoding)
# - Reports R², MAE, RMSE, MdAPE (median APE in %)
# - Visualizes one RF tree and feature importances
# - Saves best model + schema; provides predict_price()
# - Optional: evaluates on Aug 2025 holdout if dataset supports it
# ================================================

import warnings, json, math
warnings.filterwarnings("ignore")  # Keep notebooks quiet (you can disable if you want warnings)

import numpy as np
import pandas as pd
from pathlib import Path

# --- sklearn & scipy imports
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.linear_model import Ridge
from sklearn.ensemble import RandomForestRegressor
from scipy.stats import loguniform, randint

# --- persistence & plotting
import joblib
import matplotlib.pyplot as plt
from sklearn import tree

# =========================
# 1) Load the cleaned data
# =========================
# Prefer ../data when running from a notebook in /notebooks; fallback to ./data when running locally.
csv_path = "../data/cleaned_enhanced.csv" if Path("../data/cleaned_enhanced.csv").exists() else "data/cleaned_enhanced.csv"
df = pd.read_csv(csv_path)

# ===============================
# 2) Resolve target & split X / y
# ===============================
# Target resolution handles a few common header variants.
target_candidates = ["ClosePrice", "Close Price", "Close_Price", "CLOSEPRICE"]
target = next((c for c in target_candidates if c in df.columns), None)
assert target is not None, "Target column not found in cleaned_enhanced.csv"

y = pd.to_numeric(df[target], errors="coerce")
X = df.drop(columns=[target]).copy()

# Drop obvious IDs / free-text / potential leakage date columns.
DROP_LIKE = {
    "ListingKey","ListingId","Matrix_Unique_ID","UniversalPropertyId",
    "MLS","PublicRemarks","PrivateRemarks","Directions","Photos","PhotoURL",
    "ModificationTimestamp","ListingContractDate","CloseDate"
}
drop_cols = [c for c in X.columns if any(k.lower() in c.lower() for k in DROP_LIKE)]
if drop_cols:
    X.drop(columns=drop_cols, inplace=True)

# Keep rows with numeric target only.
mask = y.notna()
X, y = X.loc[mask].reset_index(drop=True), y.loc[mask].reset_index(drop=True)

# Optional: cap training size for speed in large datasets.
N_SAMPLE = min(len(X), 40_000)
if len(X) > N_SAMPLE:
    samp = X.sample(N_SAMPLE, random_state=42).index
    X, y = X.loc[samp].reset_index(drop=True), y.loc[samp].reset_index(drop=True)

# Train / test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Save expected feature schema for later predict() calls
models_path = Path("../models") if Path("../models").exists() else Path("models")
models_path.mkdir(parents=True, exist_ok=True)
with open(models_path / "expected_feature_columns.json", "w") as f:
    json.dump(X_train.columns.tolist(), f, indent=2)
print(f"Saved expected columns → {models_path / 'expected_feature_columns.json'}")

# ===================================
# 3) Column partitions (num / cat)
# ===================================
num_cols = X_train.select_dtypes(include=[np.number, "bool"]).columns.tolist()
cat_cols = X_train.select_dtypes(exclude=[np.number, "bool"]).columns.tolist()

# For Ridge, we only one-hot low-cardinality categoricals to keep the matrix compact.
CARD_LIMIT = 80
cat_low  = [c for c in cat_cols if X_train[c].nunique(dropna=True) <= CARD_LIMIT]
# High-cardinality categorical features are dropped in the Ridge path (kept in RF via ordinal encoding).

# ===================================
# 4) Preprocessors for each model
# ===================================
# Ridge: numeric + OHE for low-card cats; keep it sparse for efficiency.
# NOTE: If you're on scikit-learn >= 1.4, prefer sparse_output=True (sparse is deprecated).
pre_ridge = ColumnTransformer(
    transformers=[
        ("num", SimpleImputer(strategy="median"), num_cols),
        ("cat_low_ohe", Pipeline([
            ("impute", SimpleImputer(strategy="most_frequent")),
            ("ohe", OneHotEncoder(handle_unknown="ignore", sparse=True))  # set sparse_output=True on newer sklearn
        ]), cat_low),
    ],
    remainder="drop"
)

# Random Forest: numeric + ALL cats via OrdinalEncoder (dense). RF can handle coded categories.
pre_rf = ColumnTransformer(
    transformers=[
        ("num", SimpleImputer(strategy="median"), num_cols),
        ("cat_ord", Pipeline([
            ("impute", SimpleImputer(strategy="most_frequent")),
            ("ord", OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=-1))
        ]), cat_cols)
    ],
    remainder="drop"
)

# ================================
# 5) Models & randomized tuning
# ================================
ridge = Ridge(alpha=1.0, random_state=42)
rf = RandomForestRegressor(
    n_estimators=200,
    max_depth=18,
    min_samples_leaf=3,
    max_features="sqrt",
    n_jobs=-1,
    random_state=42
)

# Tune Ridge alpha (log-uniform)
ridge_search = RandomizedSearchCV(
    estimator=Pipeline([("prep", pre_ridge), ("model", ridge)]),
    param_distributions={"model__alpha": loguniform(1e-3, 1e3)},
    n_iter=10, cv=3, n_jobs=-1,
    scoring="neg_root_mean_squared_error", random_state=42
)
ridge_search.fit(X_train, y_train)
ridge_tuned = ridge_search.best_estimator_
print("Best Ridge alpha:", ridge_search.best_params_["model__alpha"])

# Tune Random Forest (n_estimators, depth, leaf size, feature subsampling)
rf_search = RandomizedSearchCV(
    estimator=Pipeline([("prep", pre_rf), ("model", RandomForestRegressor(random_state=42, n_jobs=-1))]),
    param_distributions={
        "model__n_estimators": randint(200, 500),
        "model__max_depth": [None, 18, 30, 40],
        "model__min_samples_leaf": randint(1, 6),
        "model__max_features": ["sqrt", "log2", None],
    },
    n_iter=15, cv=3, n_jobs=-1,
    scoring="neg_root_mean_squared_error", random_state=42
)
rf_search.fit(X_train, y_train)
rf_tuned = rf_search.best_estimator_
print("Best RF params:", rf_search.best_params_)

# =====================================
# 6) Metrics helpers + evaluation
# =====================================
def mdape(y_true, y_pred):
    """Median Absolute Percentage Error in % (robust to outliers)."""
    y_true = np.asarray(y_true).reshape(-1)
    y_pred = np.asarray(y_pred).reshape(-1)
    mask = (y_true != 0) & np.isfinite(y_true) & np.isfinite(y_pred)
    if not np.any(mask):
        return np.nan
    return float(np.median(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100)

def eval_pipe(name, pipe, X_te=X_test, y_te=y_test):
    """Evaluate a fitted pipeline on a test set and return key metrics."""
    y_pred = pipe.predict(X_te)
    return {
        "Model": name,
        "R2": float(r2_score(y_te, y_pred)),
        "MAE": float(mean_absolute_error(y_te, y_pred)),
        "RMSE": float(mean_squared_error(y_te, y_pred, squared=False)),
        "MdAPE (%)": float(mdape(y_te, y_pred)),
        "n_test": int(len(y_te)),
    }

# Evaluate tuned pipelines on the test split
ridge_m = eval_pipe("Ridge (tuned)", ridge_tuned)
rf_m    = eval_pipe("RandomForest (tuned)", rf_tuned)

# Rank by R² (desc) then RMSE (asc)
res_df = pd.DataFrame([ridge_m, rf_m]).sort_values(["R2", "RMSE"], ascending=[False, True])

# Pretty table in notebook environments
try:
    display(res_df.style.format({
        "R2": "{:.4f}", "MAE": "{:,.0f}", "RMSE": "{:,.0f}", "MdAPE (%)": "{:.2f}"
    }))
except Exception:
    pass

# Always print a plain-text summary too
print("\n=== Test Metrics Summary ===")
for row in res_df.to_dict(orient="records"):
    print(f"{row['Model']}: R2={row['R2']:.4f}, RMSE=${row['RMSE']:,.0f}, "
          f"MAE=${row['MAE']:,.0f}, MdAPE={row['MdAPE (%)']:.2f}% (n={row['n_test']})")

# =========================================
# 7) Persist results + best tuned pipeline
# =========================================
res_df.to_csv(models_path / "baseline_models_results.csv", index=False)
with open(models_path / "baseline_models_summary.json", "w") as f:
    json.dump(res_df.to_dict(orient="records"), f, indent=2)

best_row  = res_df.iloc[0]  # top-scoring row (by our sort)
best_name = best_row["Model"]
best_pipe = ridge_tuned if "Ridge" in best_name else rf_tuned

joblib.dump(best_pipe, models_path / "best_model.joblib")
print(f"Winner → {best_name}")
print("Saved:", (models_path / "best_model.joblib").resolve())

# ==================================================
# 8) Random Forest internals & feature importance
# ==================================================
# (Safe even if Ridge won—the rf_tuned object still exists for inspection.)
rf_model = rf_tuned.named_steps["model"]
print(f"\nRandomForest has {len(rf_model.estimators_)} trees")
print(f"Max depth of first tree: {rf_model.estimators_[0].tree_.max_depth}")

# Quick text dump of the first tree (depth-limited for readability)
from sklearn.tree import export_text
print("\nSample of Decision Tree #0 (max_depth=3):\n")
print(export_text(rf_model.estimators_[0], max_depth=3))

# Plot the first tree (depth-limited to keep plots light)
plt.figure(figsize=(20, 10))
tree.plot_tree(
    rf_model.estimators_[0],
    filled=True, rounded=True,
    max_depth=3, fontsize=8
)
plt.title("Random Forest — Example Decision Tree (Depth ≤ 3)")
plt.show()

# === Feature importance extraction and safety guard ===
importances = rf_model.feature_importances_

# Try to extract proper column names from the RF preprocessor
rf_pre = rf_tuned.named_steps["prep"]
try:
    feat_names_out = rf_pre.get_feature_names_out()
except Exception:
    # fallback: just use num_cols + cat_cols order
    feat_names_out = np.array(num_cols + cat_cols)

# Ensure lengths match (can happen if preprocessing drops some cols)
if len(importances) != len(feat_names_out):
    print(f"⚠️ Warning: length mismatch ({len(importances)} vs {len(feat_names_out)}). Using generic names.")
    feat_names_out = np.array([f"f{i}" for i in range(len(importances))])

# Sort and display top importances
sorted_idx = np.argsort(importances)[::-1]
top_n = min(15, len(importances))

plt.figure(figsize=(9, 6))
plt.barh(range(top_n), importances[sorted_idx[:top_n]][::-1])
plt.yticks(range(top_n), feat_names_out[sorted_idx[:top_n]][::-1])
plt.xlabel("Importance")
plt.title(f"Top {top_n} Features (Random Forest)")
plt.tight_layout()
plt.show()

print("\nTop 10 features:")
for i in range(min(10, len(importances))):
    j = sorted_idx[i]
    print(f"{feat_names_out[j]:<30} {importances[j]:.4f}")
print("Sum of importances:", importances.sum())


# ==========================
# 9) Predict API (one row)
# ==========================
with open(models_path / "expected_feature_columns.json") as f:
    EXPECTED_COLS = json.load(f)

try:
    BEST_MODEL = joblib.load(models_path / "best_model.joblib")
except Exception as e:
    print("Warning: couldn't load best_model.joblib; using rf_tuned fallback.", e)
    BEST_MODEL = rf_tuned

def predict_price(raw_features: dict, model=BEST_MODEL):
    """Build a single-row DF with expected columns, then predict."""
    row = {c: raw_features.get(c, np.nan) for c in EXPECTED_COLS}
    X_new = pd.DataFrame([row], columns=EXPECTED_COLS)
    return float(model.predict(X_new)[0])

# Example prediction (tweak values as you like)
example_house = {
    "BedroomsTotal": 3,
    "BathroomsFull": 2,
    "LivingArea": 1600,
    "LotSizeArea": 5000,
    "GarageSpaces": 2,
    "YearBuilt": 1995,
    "PostalCode": "92618"
}
price = predict_price(example_house)
print(f"\nPredicted Close Price (example): ${price:,.0f}")

# ==========================================================
# 10) Optional: Holdout evaluation for Aug 2025 (if present)
# ==========================================================
# This runs only if your dataset has a usable month field and includes Aug 2025 rows.
df_all = pd.read_csv(csv_path)

target = next((c for c in target_candidates if c in df_all.columns), None)
assert target is not None, "Target column not found."

date_candidates = [
    "CloseDate","Close Date","Close_Date","CLOSEDATE",
    "CloseOfEscrowDate","COEDate","COE",
    "ModificationTimestamp","ListAgentMlsIdDate","RecordingDate",
    "Period","period","Month","SaleMonth","month"
]
date_col = next((c for c in date_candidates if c in df_all.columns), None)

if date_col is not None:
    df_aug = df_all.copy()
    if date_col.lower() in {"period","month","salemonth"}:
        # Normalize strings like '2025-08', '202508', 'Aug-2025' → keep rows matching 202508
        s = df_aug[date_col].astype(str).str.replace(r"[^\d]", "", regex=True)
        mask_aug = (s.str.len() >= 6) & (s.str[-6:] == "202508")
    else:
        # Parse as datetime and filter year/month
        dt = pd.to_datetime(df_aug[date_col], errors="coerce", utc=False)
        mask_aug = (dt.dt.year == 2025) & (dt.dt.month == 8)

    df_aug = df_aug.loc[mask_aug].copy()

    if len(df_aug) > 0:
        X_aug = pd.DataFrame({c: df_aug.get(c, np.nan) for c in EXPECTED_COLS})
        y_aug = pd.to_numeric(df_aug[target], errors="coerce")
        mask_valid = y_aug.notna()
        X_aug, y_aug = X_aug.loc[mask_valid], y_aug.loc[mask_valid]

        if len(y_aug) > 0:
            y_pred = BEST_MODEL.predict(X_aug)
            r2   = float(r2_score(y_aug, y_pred))
            mae  = float(mean_absolute_error(y_aug, y_pred))
            rmse = float(mean_squared_error(y_aug, y_pred, squared=False))
            mda  = mdape(y_aug, y_pred)

            print(f"\nAugust 2025 evaluation — n={len(y_aug)}")
            print(f"R²:    {r2:.4f}")
            print(f"MAE:   {mae:,.0f}")
            print(f"RMSE:  {rmse:,.0f}")
            print(f"MdAPE: {mda:.2f}%")
        else:
            print("\nAugust 2025 evaluation skipped: no valid target values.")
    else:
        print("\nAugust 2025 evaluation skipped: no rows for Aug 2025 in dataset.")
else:
    print("\nHoldout evaluation skipped: no usable date/month column found.")
